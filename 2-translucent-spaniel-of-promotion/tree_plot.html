<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 6], [0, 7], [2, 5], [3, 15], [4, 12], [5, 9], [7, 10], [7, 8], [10, 11], [11, 13], [11, 17], [11, 19], [11, 18], [11, 14], [12, 16]], "layout": [[0.2, 0.0], [0.4, 0.0], [0.6, 0.0], [0.8, 0.0], [1.0, 0.0], [0.6, 0.25], [0.1, 0.25], [0.3, 0.25], [0.2, 0.5], [0.6, 0.5], [0.4, 0.5], [0.4, 0.75], [1.0, 0.25], [0.0, 1.0], [0.2, 1.0], [0.8, 0.25], [1.0, 0.5], [0.4, 1.0], [0.6, 1.0], [0.8, 1.0]], "plan": ["We will train a LightGBM model to predict the log-transformed sale price using\nsimple preprocessing: fill numeric missing values with the median and encode\ncategorical features as integer codes. We\u2019ll use 5-fold cross-validation to\nestimate performance via RMSE on the log scale. After reporting the mean CV\nscore, we will retrain on the full training set and generate predictions on the\ntest set, saving them to `submission.csv`.", "I will impute numeric features with the median and one-hot encode all\ncategorical variables, then train a Ridge regression to predict log-transformed\nsale prices. We\u2019ll evaluate using 5\u2010fold cross\u2010validation, computing the RMSE on\nthe log scale. After reporting the CV RMSE, we retrain on the full dataset and\ngenerate predictions for the test set, exporting a `submission.csv` with the\nexponentiated forecasts.", "We will impute missing numeric features with the median and label-encode all\ncategorical columns so XGBoost can handle them directly. We\u2019ll train an XGBoost\nregressor to predict the log-transformed sale price and evaluate its performance\nusing 5-fold cross-validation, computing the RMSE on the log scale. After\nreporting the average CV score, we retrain on the full training set and generate\nfinal predictions on the test set, exponentiating the outputs to return to the\noriginal scale. The submission file will be saved as submission.csv in the\nworking directory.", "We will use CatBoost to directly model log-transformed sale prices, leveraging\nits native handling of categorical features. Numeric missing values will be\nimputed by the training-set median, and we\u2019ll apply a 5-fold cross\u2010validation to\ncompute the RMSE on the log scale. After reporting the mean CV RMSE, we retrain\non the full dataset and output predictions to `submission.csv` in the working\ndirectory.", "We will impute missing numeric values with the training\u2010set median, and\nfactorize categoricals across train and test. An XGBoost regressor will be\ntrained to predict log1p(SalePrice) with early stopping, evaluated via 5\u2010fold CV\ncomputing RMSE on the log scale. After reporting the mean CV RMSE, we retrain on\nthe full data and generate test predictions, exponentiate them back, and save\nthem to submission.csv.", "We encountered the `TypeError` because our XGBoost version does not accept\n`eval_metric` in the `.fit()` call. To fix this, we move `eval_metric='rmse'`\ninto the `XGBRegressor` constructor and remove it from the `.fit()` arguments.\nThe rest of the preprocessing, 5-fold CV, and final training/prediction pipeline\nstays the same, and we print the CV RMSE before saving the submission.", "I propose to reduce skewness in continuous features by applying a log1p\ntransformation to those with high skew (>0.75), which often improves convergence\nand accuracy of tree\u2010based models. We first identify original numeric columns,\nimpute missing values with the median, and encode categorical variables as\ninteger codes. Then we compute skewness on the training set and apply log1p to\nboth train and test for skewed features. Finally, we retrain the LightGBM model\nwith unchanged parameters using 5\u2010fold CV to measure the impact on RMSE and\ngenerate test predictions.", "We will engineer a new feature \"TotalSF\" by summing \"TotalBsmtSF\", \"1stFlrSF\",\nand \"2ndFlrSF\", filling missing \"TotalBsmtSF\" as zero to reflect no basement.\nThe rest of the preprocessing (median imputation for numeric, label encoding for\ncategoricals) remains the same. We will then train the LightGBM model with\n5-fold CV to measure the improvement and generate our final submission.", "I will engineer a new feature \"TotalBath\" by summing full baths and half baths\n(counting each half bath as 0.5) across basement and above\u2010ground levels, which\noften correlates strongly with overall house quality and size. This is an atomic\nchange building on the previous TotalSF feature, and it should improve the\nLightGBM model\u2019s ability to predict sale prices. I will then perform the same\npreprocessing, 5\u2010fold CV with LightGBM, report the CV RMSE, and produce a\nsubmission file.", "We can fix the early stopping bug by using XGBoost\u2019s callback API instead of the\ndeprecated `early_stopping_rounds` parameter. We replace\n`early_stopping_rounds=50` with\n`callbacks=[xgb.callback.EarlyStopping(rounds=50, save_best=True)]` in the `fit`\ncall. The rest of the pipeline remains the same: data loading,\nlog\u2010transformation, encoding, 5\u2010fold CV with rmse evaluation, printing the CV\nscore, final training on full data, predicting on test set, inverting the log\ntransform, and saving `submission.csv`.", "I will augment the feature set with three new \u201cage\u201d features: HouseAge\n(YrSold\u2212YearBuilt), SinceRemodel (YrSold\u2212YearRemodAdd), and GarageAge\n(YrSold\u2212GarageYrBlt, treating missing GarageYrBlt as YrSold). These capture the\nproperty\u2019s age, time since remodel, and garage age, which should help the model.\nI will then follow the existing pipeline: median imputation, label encoding,\nLightGBM with 5-fold CV, and output CV RMSE and test predictions.", "I propose adding a new \u201cTotalPorchSF\u201d feature by summing OpenPorchSF,\nEnclosedPorch, 3SsnPorch, and ScreenPorch, which captures overall porch area\ncontributing to a house\u2019s value. This feature is appended to both train and test\ndata before the existing median imputation and label encoding. We then retrain\nthe LightGBM model with 5\u2010fold CV to measure the impact on log\u2010scale RMSE and\ngenerate the final submission.", "We can fix the TypeError by moving `eval_metric` into the `XGBRegressor`\nconstructor instead of passing it to `fit`, which ensures compatibility with the\ninstalled xgboost version. The rest of the preprocessing and 5-fold\ncross\u2010validation setup remains unchanged, with early stopping based on the\nvalidation set. After computing and printing the average CV RMSE on the log\nscale, we retrain on the full dataset using a scaled `n_estimators` from the\nlast fold\u2019s best iteration and then generate `submission.csv`.", "I will apply out-of-fold target mean encoding to the Neighborhood feature to\nbetter capture its predictive power while avoiding leakage. For each CV fold, I\ncompute the average log\u2010sale price for each neighborhood on the training split\nand map it to the validation and test sets, filling unseen categories with the\nglobal mean. This creates a new continuous Neighborhood_TE feature that is then\nused by LightGBM along with the existing features. Finally, I recompute the\nfull\u2010data mapping, train on the entire set, encode the test set, and generate\nthe `submission.csv`.", "I will augment the existing pipeline by adding a TotalBath feature that sums all\nfull and half baths\u2014treating half baths as 0.5\u2014and filling missing basement bath\ncounts as zero. This captures the overall bathroom count, which is strongly\npredictive of house value. I will compute this feature for both train and test\nbefore median imputation and label encoding, then rerun 5-fold CV with LightGBM\nto report the updated RMSE, and finally train on the full data and save\npredictions.", "We switch from CatBoost (which isn\u2019t installed) to LightGBM, encoding\ncategorical variables with LabelEncoder fitted on both train and test to avoid\nunseen labels. We otherwise keep the same 5\u2010fold CV using log1p targets, compute\nRMSE on log scale, then retrain on full data and write predictions (expm1) to\nsubmission.csv. Categorical features are passed to LightGBM via their column\nindices.", "We\u2019ll switch from the sklearn wrapper to the native XGBoost API so that we can\nuse early stopping via xgb.train.  For each fold we\u2019ll build DMatrix objects,\ncall xgb.train with evals and early_stopping_rounds, record the best iteration,\nand compute the RMSE on the log\u2010scale.  We\u2019ll then retrain on all data using the\naverage best iteration (scaled by 1.1), predict the test set, and save\n`submission.csv`.  Finally, we\u2019ll print the CV RMSE in log\u2010scale.", "I propose adding a frequency\u2010encoding feature for the Neighborhood variable to\ncapture its relative prevalence in the training data without leaking sale price\ninformation. We compute the frequency of each neighborhood in the training set\nand map it to both train and test as a new numeric feature `Neighborhood_FE`.\nThis additional information may help LightGBM discern neighborhood effects more\neffectively. We then follow the existing pipeline: median imputation, label\nencoding, 5\u2010fold CV for RMSE evaluation, and full\u2010data retraining for\nsubmission.", "I will impute missing LotFrontage values by the median LotFrontage of each\nNeighborhood, falling back to the overall median when needed. This leverages\nspatial information to better estimate lot frontage and reduce noise. After this\ntargeted imputation, I will apply the existing feature engineering pipeline\n(TotalSF, age features, TotalPorchSF), then perform median imputation for\nremaining numeric gaps and label\u2010encode categoricals. Finally, I will run 5\u2010fold\nCV with LightGBM, print the log\u2010scale RMSE, retrain on all data, and save the\ntest predictions to `submission.csv`.", "I will map ordinal quality features (e.g., ExterQual, KitchenQual, FireplaceQu)\nto explicit numeric scales reflecting their inherent ranking, filling missing\nvalues with zero, rather than using arbitrary label encoding. This should help\nthe model interpret these assessments correctly and improve performance. All\nother preprocessing and feature engineering steps remain unchanged, and we'll\nretrain with 5\u2010fold CV to evaluate the impact and generate the final submission."], "code": ["import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1)\nX_test = test.drop(\"Id\", axis=1)\n\n# Simple preprocessing\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Separate target and drop Id\ny = np.log1p(train[\"SalePrice\"])\ntrain = train.drop([\"Id\", \"SalePrice\"], axis=1)\ntest = test.drop([\"Id\"], axis=1)\n\n# Identify numeric and categorical features\nnumeric_feats = train.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_feats = train.select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Impute numeric features with median\nfor col in numeric_feats:\n    median = train[col].median()\n    train[col].fillna(median, inplace=True)\n    test[col].fillna(median, inplace=True)\n\n# Fill missing categoricals and one-hot encode\ntrain[categorical_feats] = train[categorical_feats].fillna(\"Missing\")\ntest[categorical_feats] = test[categorical_feats].fillna(\"Missing\")\ntrain_enc = pd.get_dummies(train, columns=categorical_feats)\ntest_enc = pd.get_dummies(test, columns=categorical_feats)\n\n# Align train/test\ntrain_enc, test_enc = train_enc.align(test_enc, join=\"left\", axis=1, fill_value=0)\n\nX = train_enc.values\nX_test = test_enc.values\n\n# 5-fold CV RMSE on log scale\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmses = []\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = Ridge(alpha=1.0, random_state=42)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    rmse = mean_squared_error(y_val, preds, squared=False)\n    rmses.append(rmse)\n\nprint(f\"CV RMSE (log scale): {np.mean(rmses):.5f}\")\n\n# Retrain on full data and predict test\nfinal_model = Ridge(alpha=1.0, random_state=42)\nfinal_model.fit(X, y)\ntest_preds_log = final_model.predict(X_test)\ntest_preds = np.expm1(test_preds_log)\n\n# Save submission\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Log-transform target\ny = np.log1p(train[\"SalePrice\"].values)\n\n# Combine for consistent encoding\nall_data = pd.concat([train.drop(columns=[\"SalePrice\"]), test], sort=False).reset_index(\n    drop=True\n)\n\n# Identify columns\nid_col = \"Id\"\ncat_cols = all_data.select_dtypes(include=[\"object\"]).columns.tolist()\nnum_cols = [c for c in all_data.columns if c not in cat_cols + [id_col]]\n\n# Impute numeric\nfor c in num_cols:\n    median = all_data[c].median()\n    all_data[c] = all_data[c].fillna(median)\n\n# Label-encode categorical\nfor c in cat_cols:\n    all_data[c] = all_data[c].fillna(\"NA\")\n    all_data[c], _ = pd.factorize(all_data[c])\n\n# Split back\nX = all_data.iloc[: train.shape[0], :].drop(columns=[id_col])\nX_test = all_data.iloc[train.shape[0] :, :].drop(columns=[id_col])\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_list = []\nfor train_idx, valid_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_val = y[train_idx], y[valid_idx]\n    model = xgb.XGBRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        verbosity=0,\n    )\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    preds = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, preds))\n    rmse_list.append(rmse)\n\ncv_score = np.mean(rmse_list)\nprint(f\"CV RMSE: {cv_score:.5f}\")\n\n# Train on full data\nfinal_model = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    verbosity=0,\n)\nfinal_model.fit(X, y)\n\n# Predict and invert log\ntest_preds = final_model.predict(X_test)\ntest_preds = np.expm1(test_preds)\n\n# Save submission\nsubmission = pd.DataFrame({\"Id\": test[id_col], \"SalePrice\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Prepare target\ny = np.log1p(train[\"SalePrice\"])\n\n# Identify features\ndrop_cols = [\"Id\", \"SalePrice\"]\nfeatures = [c for c in train.columns if c not in drop_cols]\n# Separate numeric vs categorical\nnum_cols = train[features].select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = train[features].select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Impute numeric missing with median\nmedians = train[num_cols].median()\ntrain[num_cols] = train[num_cols].fillna(medians)\ntest[num_cols] = test[num_cols].fillna(medians)\n\n# For simplicity, fill categorical missing with a placeholder\ntrain[cat_cols] = train[cat_cols].fillna(\"MISSING\")\ntest[cat_cols] = test[cat_cols].fillna(\"MISSING\")\n\nX = train[features]\nX_test = test[features]\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_scores = []\n\nfor train_idx, valid_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n\n    model = CatBoostRegressor(\n        iterations=1000,\n        learning_rate=0.05,\n        depth=6,\n        loss_function=\"RMSE\",\n        eval_metric=\"RMSE\",\n        random_seed=42,\n        logging_level=\"Silent\",\n        early_stopping_rounds=50,\n    )\n    model.fit(\n        X_tr, y_tr, eval_set=(X_val, y_val), cat_features=cat_cols, use_best_model=True\n    )\n    pred_val = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n    rmse_scores.append(rmse)\n\ncv_rmse = np.mean(rmse_scores)\nprint(f\"CV RMSE (log-scale): {cv_rmse:.5f}\")\n\n# Retrain on full data\nfinal_model = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=6,\n    loss_function=\"RMSE\",\n    random_seed=42,\n    logging_level=\"Silent\",\n)\nfinal_model.fit(X, y, cat_features=cat_cols)\n\n# Predict and write submission\npred_test_log = final_model.predict(X_test)\npred_test = np.expm1(pred_test_log)\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": pred_test})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntrain_ids = train[\"Id\"]\ntest_ids = test[\"Id\"]\n\n# Separate target\ny = np.log1p(train[\"SalePrice\"])\n\n# Combine for preprocessing\ntrain_feat = train.drop([\"Id\", \"SalePrice\"], axis=1)\ntest_feat = test.drop([\"Id\"], axis=1)\nall_feat = pd.concat([train_feat, test_feat], axis=0).reset_index(drop=True)\n\n# Impute numeric features\nnum_cols = all_feat.select_dtypes(include=[np.number]).columns\nmedians = train_feat[num_cols].median()\nall_feat[num_cols] = all_feat[num_cols].fillna(medians)\n\n# Encode categorical features\ncat_cols = all_feat.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    all_feat[col] = all_feat[col].fillna(\"NA\").astype(str)\n    all_feat[col], _ = pd.factorize(all_feat[col])\n\n# Split back\nX = all_feat.iloc[: len(train), :].values\nX_test = all_feat.iloc[len(train) :, :].values\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmses = []\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = XGBRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    preds_val = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n    rmses.append(rmse)\n\n# Report CV score\ncv_score = np.mean(rmses)\nprint(f\"CV RMSE (log scale): {cv_score:.5f}\")\n\n# Retrain on full data\nfinal_model = XGBRegressor(\n    n_estimators=int(model.best_iteration * 1.1),\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model.fit(X, y, verbose=False)\n\n# Predict and save submission\npreds_test_log = final_model.predict(X_test)\npreds_test = np.expm1(preds_test_log)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": preds_test})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Log-transform target\ny = np.log1p(train[\"SalePrice\"].values)\n\n# Combine for consistent encoding\nall_data = pd.concat([train.drop(columns=[\"SalePrice\"]), test], sort=False).reset_index(\n    drop=True\n)\n\n# Identify columns\nid_col = \"Id\"\ncat_cols = all_data.select_dtypes(include=[\"object\"]).columns.tolist()\nnum_cols = [c for c in all_data.columns if c not in cat_cols + [id_col]]\n\n# Impute numeric columns with median\nfor c in num_cols:\n    median = all_data[c].median()\n    all_data[c] = all_data[c].fillna(median)\n\n# Label-encode categorical columns\nfor c in cat_cols:\n    all_data[c] = all_data[c].fillna(\"NA\")\n    all_data[c], _ = pd.factorize(all_data[c])\n\n# Split back into train and test sets\nX = all_data.iloc[: train.shape[0], :].drop(columns=[id_col])\nX_test = all_data.iloc[train.shape[0] :, :].drop(columns=[id_col])\n\n# 5-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_list = []\nfor train_idx, valid_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_val = y[train_idx], y[valid_idx]\n    model = xgb.XGBRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        verbosity=0,\n        eval_metric=\"rmse\",\n    )\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    preds = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, preds))\n    rmse_list.append(rmse)\n\ncv_score = np.mean(rmse_list)\nprint(f\"CV RMSE: {cv_score:.5f}\")\n\n# Train on full data\nfinal_model = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    verbosity=0,\n    eval_metric=\"rmse\",\n)\nfinal_model.fit(X, y)\n\n# Predict on test set and invert log\ntest_preds = final_model.predict(X_test)\ntest_preds = np.expm1(test_preds)\n\n# Save submission\nsubmission = pd.DataFrame({\"Id\": test[id_col], \"SalePrice\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain_df = pd.read_csv(\"./input/train.csv\")\ntest_df = pd.read_csv(\"./input/test.csv\")\ntest_ids = test_df[\"Id\"]\n\n# Identify feature types\ncat_feats = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\nnumeric_feats = [\n    c for c in train_df.columns if c not in cat_feats + [\"Id\", \"SalePrice\"]\n]\n\n# Prepare features and target\ny = np.log1p(train_df[\"SalePrice\"])\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test_df.drop(\"Id\", axis=1).copy()\n\n# Simple preprocessing: impute and encode\nfor col in numeric_feats:\n    med = X[col].median()\n    X[col] = X[col].fillna(med)\n    X_test[col] = X_test[col].fillna(med)\nfor col in cat_feats:\n    X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n\n# Log-transform highly skewed numeric features\nskewness = X[numeric_feats].skew()\nskewed_feats = skewness[skewness.abs() > 0.75].index.tolist()\nfor col in skewed_feats:\n    X[col] = np.log1p(X[col])\n    X_test[col] = np.log1p(X_test[col])\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering: fill missing TotalBsmtSF as 0 then create TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Target\ny = np.log1p(train[\"SalePrice\"])\n\n# Drop id and target\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering: TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Feature engineering: TotalBath = full baths + 0.5*half baths (basement + above)\nfor df in [X, X_test]:\n    df[\"FullBath\"] = df[\"FullBath\"].fillna(0)\n    df[\"HalfBath\"] = df[\"HalfBath\"].fillna(0)\n    df[\"BsmtFullBath\"] = df[\"BsmtFullBath\"].fillna(0)\n    df[\"BsmtHalfBath\"] = df[\"BsmtHalfBath\"].fillna(0)\n    df[\"TotalBath\"] = (\n        df[\"FullBath\"]\n        + df[\"BsmtFullBath\"]\n        + 0.5 * (df[\"HalfBath\"] + df[\"BsmtHalfBath\"])\n    )\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42, n_jobs=-1)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42, n_jobs=-1)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Log-transform target\ny = np.log1p(train[\"SalePrice\"].values)\n\n# Combine for consistent encoding\nall_data = pd.concat([train.drop(columns=[\"SalePrice\"]), test], sort=False).reset_index(\n    drop=True\n)\n\n# Identify columns\nid_col = \"Id\"\ncat_cols = all_data.select_dtypes(include=[\"object\"]).columns.tolist()\nnum_cols = [c for c in all_data.columns if c not in cat_cols + [id_col]]\n\n# Impute numeric columns with median\nfor c in num_cols:\n    all_data[c] = all_data[c].fillna(all_data[c].median())\n\n# Label-encode categorical columns\nfor c in cat_cols:\n    all_data[c] = all_data[c].fillna(\"NA\")\n    all_data[c], _ = pd.factorize(all_data[c])\n\n# Split back into train and test sets\nX = all_data.iloc[: train.shape[0], :].drop(columns=[id_col])\nX_test = all_data.iloc[train.shape[0] :, :].drop(columns=[id_col])\n\n# 5-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_list = []\n\nfor train_idx, valid_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_val = y[train_idx], y[valid_idx]\n    model = xgb.XGBRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        verbosity=0,\n        eval_metric=\"rmse\",\n    )\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False,\n        callbacks=[xgb.callback.EarlyStopping(rounds=50, save_best=True)],\n    )\n    preds = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, preds))\n    rmse_list.append(rmse)\n\ncv_score = np.mean(rmse_list)\nprint(f\"CV RMSE: {cv_score:.5f}\")\n\n# Train on full data\nfinal_model = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    verbosity=0,\n    eval_metric=\"rmse\",\n)\nfinal_model.fit(X, y)\n\n# Predict on test set and invert log\ntest_preds = final_model.predict(X_test)\ntest_preds = np.expm1(test_preds)\n\n# Save submission\nsubmission = pd.DataFrame({\"Id\": test[id_col], \"SalePrice\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering: fill missing TotalBsmtSF as 0 then create TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Create age-based features\n# Treat missing GarageYrBlt as YrSold => age zero\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].fillna(X[\"YrSold\"])\nX_test[\"GarageYrBlt\"] = X_test[\"GarageYrBlt\"].fillna(X_test[\"YrSold\"])\nX[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\nX_test[\"HouseAge\"] = X_test[\"YrSold\"] - X_test[\"YearBuilt\"]\nX[\"SinceRemodel\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\nX_test[\"SinceRemodel\"] = X_test[\"YrSold\"] - X_test[\"YearRemodAdd\"]\nX[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"]\nX_test[\"GarageAge\"] = X_test[\"YrSold\"] - X_test[\"GarageYrBlt\"]\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering: fill missing TotalBsmtSF as 0 then create TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Create age-based features\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].fillna(X[\"YrSold\"])\nX_test[\"GarageYrBlt\"] = X_test[\"GarageYrBlt\"].fillna(X_test[\"YrSold\"])\nX[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\nX_test[\"HouseAge\"] = X_test[\"YrSold\"] - X_test[\"YearBuilt\"]\nX[\"SinceRemodel\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\nX_test[\"SinceRemodel\"] = X_test[\"YrSold\"] - X_test[\"YearRemodAdd\"]\nX[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"]\nX_test[\"GarageAge\"] = X_test[\"YrSold\"] - X_test[\"GarageYrBlt\"]\n\n# New feature: total porch square footage\nporch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nX[\"TotalPorchSF\"] = X[porch_cols].sum(axis=1)\nX_test[\"TotalPorchSF\"] = X_test[porch_cols].sum(axis=1)\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntrain_ids = train[\"Id\"]\ntest_ids = test[\"Id\"]\n\n# Separate target\ny = np.log1p(train[\"SalePrice\"])\n\n# Combine for preprocessing\ntrain_feat = train.drop([\"Id\", \"SalePrice\"], axis=1)\ntest_feat = test.drop([\"Id\"], axis=1)\nall_feat = pd.concat([train_feat, test_feat], axis=0).reset_index(drop=True)\n\n# Impute numeric features\nnum_cols = all_feat.select_dtypes(include=[np.number]).columns\nmedians = train_feat[num_cols].median()\nall_feat[num_cols] = all_feat[num_cols].fillna(medians)\n\n# Encode categorical features\ncat_cols = all_feat.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    all_feat[col] = all_feat[col].fillna(\"NA\").astype(str)\n    all_feat[col], _ = pd.factorize(all_feat[col])\n\n# Split back\nX = all_feat.iloc[: len(train), :].values\nX_test = all_feat.iloc[len(train) :, :].values\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmses = []\nbest_iters = []\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = XGBRegressor(\n        n_estimators=1000,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n        eval_metric=\"rmse\",\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    preds_val = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n    rmses.append(rmse)\n    best_iters.append(model.best_iteration)\n\n# Report CV score\ncv_score = np.mean(rmses)\nprint(f\"CV RMSE (log scale): {cv_score:.5f}\")\n\n# Retrain on full data\navg_best_iter = int(np.mean(best_iters) * 1.1)\nfinal_model = XGBRegressor(\n    n_estimators=avg_best_iter,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    n_jobs=-1,\n    eval_metric=\"rmse\",\n)\nfinal_model.fit(X, y, verbose=False)\n\n# Predict and save submission\npreds_test_log = final_model.predict(X_test)\npreds_test = np.expm1(preds_test_log)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": preds_test})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Target\ny = np.log1p(train[\"SalePrice\"])\n\n# Basic feature matrices\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering\nfor df in (X, X_test):\n    df[\"TotalBsmtSF\"] = df[\"TotalBsmtSF\"].fillna(0)\n    df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(df[\"YrSold\"])\n    df[\"HouseAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n    df[\"SinceRemodel\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    df[\"GarageAge\"] = df[\"YrSold\"] - df[\"GarageYrBlt\"]\n    porch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\n    df[\"TotalPorchSF\"] = df[porch_cols].sum(axis=1)\n\n# Separate Neighborhood for target encoding\nneigh = X[\"Neighborhood\"].copy()\nneigh_test = X_test[\"Neighborhood\"].copy()\nX = X.drop(\"Neighborhood\", axis=1)\nX_test = X_test.drop(\"Neighborhood\", axis=1)\n\n# Preprocessing: median impute numeric, label encode other categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# Prepare arrays for out-of-fold predictions and test encodings\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nneigh_te_test_folds = np.zeros((5, len(X_test)))\n\n# CV with out-of-fold target encoding for Neighborhood\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n    X_tr, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    # mapping on train fold\n    mapping = y_tr.groupby(neigh.iloc[train_idx]).mean()\n    global_mean = y_tr.mean()\n    # encode\n    X_tr[\"Neighborhood_TE\"] = neigh.iloc[train_idx].map(mapping).fillna(global_mean)\n    X_val[\"Neighborhood_TE\"] = neigh.iloc[val_idx].map(mapping).fillna(global_mean)\n    neigh_te_test_folds[fold, :] = neigh_test.map(mapping).fillna(global_mean)\n    # train\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Prepare full-data encoding for final model\nmapping_full = y.groupby(neigh).mean()\nglobal_mean_full = y.mean()\nX_full = X.copy()\nX_full[\"Neighborhood_TE\"] = neigh.map(mapping_full).fillna(global_mean_full)\nX_test_full = X_test.copy()\nX_test_full[\"Neighborhood_TE\"] = neigh_test.map(mapping_full).fillna(global_mean_full)\n\n# Train final model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X_full, y)\ntest_preds = final_model.predict(X_test_full)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Existing feature engineering\n# TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Age-based features\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].fillna(X[\"YrSold\"])\nX_test[\"GarageYrBlt\"] = X_test[\"GarageYrBlt\"].fillna(X_test[\"YrSold\"])\nX[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\nX_test[\"HouseAge\"] = X_test[\"YrSold\"] - X_test[\"YearBuilt\"]\nX[\"SinceRemodel\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\nX_test[\"SinceRemodel\"] = X_test[\"YrSold\"] - X_test[\"YearRemodAdd\"]\nX[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"]\nX_test[\"GarageAge\"] = X_test[\"YrSold\"] - X_test[\"GarageYrBlt\"]\n\n# Porches\nporch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nX[\"TotalPorchSF\"] = X[porch_cols].sum(axis=1)\nX_test[\"TotalPorchSF\"] = X_test[porch_cols].sum(axis=1)\n\n# New feature: TotalBath\n# Fill missing basement bath counts as zero\nfor col in [\"BsmtFullBath\", \"BsmtHalfBath\"]:\n    X[col] = X[col].fillna(0)\n    X_test[col] = X_test[col].fillna(0)\nX[\"TotalBath\"] = (\n    X[\"FullBath\"] + 0.5 * X[\"HalfBath\"] + X[\"BsmtFullBath\"] + 0.5 * X[\"BsmtHalfBath\"]\n)\nX_test[\"TotalBath\"] = (\n    X_test[\"FullBath\"]\n    + 0.5 * X_test[\"HalfBath\"]\n    + X_test[\"BsmtFullBath\"]\n    + 0.5 * X_test[\"BsmtHalfBath\"]\n)\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Target\ny = np.log1p(train[\"SalePrice\"])\n\n# Features to drop\ndrop_cols = [\"Id\", \"SalePrice\"]\nfeatures = [c for c in train.columns if c not in drop_cols]\n\n# Identify numerical and categorical\nnum_cols = train[features].select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = train[features].select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Impute numerical\nmedians = train[num_cols].median()\ntrain[num_cols] = train[num_cols].fillna(medians)\ntest[num_cols] = test[num_cols].fillna(medians)\n\n# Impute categorical\ntrain[cat_cols] = train[cat_cols].fillna(\"MISSING\")\ntest[cat_cols] = test[cat_cols].fillna(\"MISSING\")\n\n# Encode categoricals with LabelEncoder fitted on both\nfor col in cat_cols:\n    le = LabelEncoder()\n    combined = pd.concat([train[col], test[col]], axis=0).astype(str)\n    le.fit(combined)\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\nX = train[features]\nX_test = test[features]\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmse_scores = []\nfor train_idx, valid_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n\n    model = lgb.LGBMRegressor(\n        n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42\n    )\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    pred_val = model.predict(X_val, num_iteration=model.best_iteration_)\n    rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n    rmse_scores.append(rmse)\n\ncv_rmse = np.mean(rmse_scores)\nprint(f\"CV RMSE (log-scale): {cv_rmse:.5f}\")\n\n# Retrain on full data\nfinal_model = lgb.LGBMRegressor(\n    n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42\n)\nfinal_model.fit(X, y, verbose=False)\n\n# Predict and write submission\npred_test_log = final_model.predict(X_test)\npred_test = np.expm1(pred_test_log)\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": pred_test})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntrain_ids = train[\"Id\"]\ntest_ids = test[\"Id\"]\n\n# Separate target\ny = np.log1p(train[\"SalePrice\"])\n\n# Combine for preprocessing\ntrain_feat = train.drop([\"Id\", \"SalePrice\"], axis=1)\ntest_feat = test.drop([\"Id\"], axis=1)\nall_feat = pd.concat([train_feat, test_feat], axis=0).reset_index(drop=True)\n\n# Impute numeric features\nnum_cols = all_feat.select_dtypes(include=[np.number]).columns\nmedians = train_feat[num_cols].median()\nall_feat[num_cols] = all_feat[num_cols].fillna(medians)\n\n# Encode categorical features\ncat_cols = all_feat.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    all_feat[col] = all_feat[col].fillna(\"NA\").astype(str)\n    all_feat[col], _ = pd.factorize(all_feat[col])\n\n# Split back\nX = all_feat.iloc[: len(train), :].values\nX_test = all_feat.iloc[len(train) :, :].values\n\n# XGBoost parameters\nparams = {\n    \"objective\": \"reg:squarederror\",\n    \"eta\": 0.05,\n    \"max_depth\": 6,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"seed\": 42,\n    \"eval_metric\": \"rmse\",\n}\n\n# 5-fold CV with native API\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nrmses = []\nbest_rounds = []\n\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y.iloc[train_idx].values, y.iloc[val_idx].values\n\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval = xgb.DMatrix(X_val, label=y_val)\n\n    evallist = [(dval, \"eval\")]\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=1000,\n        evals=evallist,\n        early_stopping_rounds=50,\n        verbose_eval=False,\n    )\n\n    best_it = model.best_iteration\n    best_rounds.append(best_it)\n\n    preds_val = model.predict(dval, ntree_limit=best_it)\n    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n    rmses.append(rmse)\n\n# Report CV score\ncv_score = np.mean(rmses)\nprint(f\"CV RMSE (log scale): {cv_score:.5f}\")\n\n# Retrain on full data\navg_best_it = int(np.mean(best_rounds) * 1.1)\ndfull = xgb.DMatrix(X, label=y.values)\nfinal_model = xgb.train(params, dfull, num_boost_round=avg_best_it, verbose_eval=False)\n\n# Predict and save submission\ndtest = xgb.DMatrix(X_test)\npreds_test_log = final_model.predict(dtest, ntree_limit=avg_best_it)\npreds_test = np.expm1(preds_test_log)\n\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": preds_test})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Frequency Encode Neighborhood\nneigh_freq = X[\"Neighborhood\"].value_counts(normalize=True)\nX[\"Neighborhood_FE\"] = X[\"Neighborhood\"].map(neigh_freq)\nX_test[\"Neighborhood_FE\"] = X_test[\"Neighborhood\"].map(neigh_freq).fillna(0)\n\n# Feature engineering: fill missing TotalBsmtSF as 0 then create TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Create age-based features\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].fillna(X[\"YrSold\"])\nX_test[\"GarageYrBlt\"] = X_test[\"GarageYrBlt\"].fillna(X_test[\"YrSold\"])\nX[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\nX_test[\"HouseAge\"] = X_test[\"YrSold\"] - X_test[\"YearBuilt\"]\nX[\"SinceRemodel\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\nX_test[\"SinceRemodel\"] = X_test[\"YrSold\"] - X_test[\"YearRemodAdd\"]\nX[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"]\nX_test[\"GarageAge\"] = X_test[\"YrSold\"] - X_test[\"GarageYrBlt\"]\n\n# New feature: total porch square footage\nporch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nX[\"TotalPorchSF\"] = X[porch_cols].sum(axis=1)\nX_test[\"TotalPorchSF\"] = X_test[porch_cols].sum(axis=1)\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Impute LotFrontage by Neighborhood median\nlf_median = X.groupby(\"Neighborhood\")[\"LotFrontage\"].median()\nX[\"LotFrontage\"] = X[\"LotFrontage\"].fillna(X[\"Neighborhood\"].map(lf_median))\nX_test[\"LotFrontage\"] = X_test[\"LotFrontage\"].fillna(\n    X_test[\"Neighborhood\"].map(lf_median)\n)\nglobal_median_lf = X[\"LotFrontage\"].median()\nX[\"LotFrontage\"] = X[\"LotFrontage\"].fillna(global_median_lf)\nX_test[\"LotFrontage\"] = X_test[\"LotFrontage\"].fillna(global_median_lf)\n\n# Feature engineering: TotalSF\nX[\"TotalBsmtSF\"] = X[\"TotalBsmtSF\"].fillna(0)\nX_test[\"TotalBsmtSF\"] = X_test[\"TotalBsmtSF\"].fillna(0)\nX[\"TotalSF\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\nX_test[\"TotalSF\"] = X_test[\"TotalBsmtSF\"] + X_test[\"1stFlrSF\"] + X_test[\"2ndFlrSF\"]\n\n# Age features\nX[\"GarageYrBlt\"] = X[\"GarageYrBlt\"].fillna(X[\"YrSold\"])\nX_test[\"GarageYrBlt\"] = X_test[\"GarageYrBlt\"].fillna(X_test[\"YrSold\"])\nX[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\nX_test[\"HouseAge\"] = X_test[\"YrSold\"] - X_test[\"YearBuilt\"]\nX[\"SinceRemodel\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\nX_test[\"SinceRemodel\"] = X_test[\"YrSold\"] - X_test[\"YearRemodAdd\"]\nX[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"]\nX_test[\"GarageAge\"] = X_test[\"YrSold\"] - X_test[\"GarageYrBlt\"]\n\n# Total porch square footage\nporch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nX[\"TotalPorchSF\"] = X[porch_cols].sum(axis=1)\nX_test[\"TotalPorchSF\"] = X_test[porch_cols].sum(axis=1)\n\n# Preprocessing: median impute numeric, label encode categorical\nfor col in X.columns:\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Prepare features and target\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop([\"Id\", \"SalePrice\"], axis=1).copy()\nX_test = test.drop(\"Id\", axis=1).copy()\n\n# Feature engineering\nfor df in [X, X_test]:\n    # TotalSF\n    df[\"TotalBsmtSF\"] = df[\"TotalBsmtSF\"].fillna(0)\n    df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n    # Age features\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(df[\"YrSold\"])\n    df[\"HouseAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n    df[\"SinceRemodel\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    df[\"GarageAge\"] = df[\"YrSold\"] - df[\"GarageYrBlt\"]\n    # Porch\n    porch_cols = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\n    df[\"TotalPorchSF\"] = df[porch_cols].sum(axis=1)\n\n# Ordinal quality mappings\nqual_mapping = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1}\nordinal_cols = [\n    \"ExterQual\",\n    \"ExterCond\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"FireplaceQu\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n]\nfor col in ordinal_cols:\n    X[col] = X[col].map(qual_mapping).fillna(0)\n    X_test[col] = X_test[col].map(qual_mapping).fillna(0)\n\n# Preprocessing: median impute numeric, label encode categoricals\nfor col in X.columns:\n    if col in ordinal_cols:  # already numeric\n        continue\n    if X[col].dtype == \"object\":\n        X[col] = X[col].fillna(\"Missing\").astype(\"category\").cat.codes\n        X_test[col] = X_test[col].fillna(\"Missing\").astype(\"category\").cat.codes\n    else:\n        med = X[col].median()\n        X[col] = X[col].fillna(med)\n        X_test[col] = X_test[col].fillna(med)\n\n# 5-fold CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMRegressor(random_state=42)\n    model.fit(X_tr, y_tr)\n    oof_preds[val_idx] = model.predict(X_val)\n\n# Evaluate\nrmse = np.sqrt(mean_squared_error(y, oof_preds))\nprint(f\"CV RMSE: {rmse:.5f}\")\n\n# Train full model and predict test\nfinal_model = LGBMRegressor(random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": np.expm1(test_preds)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)"], "term_out": ["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3126\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 73\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000511 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3122\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3107\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 72\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000536 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3125\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000210 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3124\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 73\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13718\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3387\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 74\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "CV RMSE (log scale): 0.14813\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 53, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_train,\n        ^^^^^^^^\n    ...<4 lines>...\n        verbose=False,\n        ^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: XGBModel.fit() got an unexpected keyword argument 'eval_metric'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 6, in <module>\n    from catboost import CatBoostRegressor\nModuleNotFoundError: No module named 'catboost'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 51, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_tr,\n        ^^^^^\n    ...<4 lines>...\n        verbose=False,\n        ^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: XGBModel.fit() got an unexpected keyword argument 'eval_metric'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 54, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_train,\n        ^^^^^^^^\n    ...<3 lines>...\n        verbose=False,\n        ^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3126\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 73\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000565 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3122\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000546 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3107\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 72\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3125\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3124\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 73\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13721\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3387\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 74\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000541 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3381\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3377\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 75\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000431 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3362\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 73\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3380\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 75\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000686 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3379\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13505\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000634 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3642\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 75\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a second seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000625 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3391\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 75\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3387\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000637 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3372\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 74\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000662 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3389\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3389\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 75\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000712 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3652\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 76\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 54, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_train,\n        ^^^^^^^^\n    ...<3 lines>...\n        callbacks=[xgb.callback.EarlyStopping(rounds=50, save_best=True)],\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: XGBModel.fit() got an unexpected keyword argument 'callbacks'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3634\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3629\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3611\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 76\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3627\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000621 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3627\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13447\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3901\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 78\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000640 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3876\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000602 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3864\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000578 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3853\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3858\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3862\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13407\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000327 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4150\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 79\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 53, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_tr,\n        ^^^^^\n    ...<3 lines>...\n        verbose=False,\n        ^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000627 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3877\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000732 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3865\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3854\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000571 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3859\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000560 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3863\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13564\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4151\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 79\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3886\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3874\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3863\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000606 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3867\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3872\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13557\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000642 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4160\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 80\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 54, in <module>\n    model.fit(\n    ~~~~~~~~~^\n        X_tr,\n        ^^^^^\n    ...<4 lines>...\n        verbose=False,\n        ^^^^^^^^^^^^^^\n    )\n    ^\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 72, in <module>\n    preds_val = model.predict(dval, ntree_limit=best_it)\n  File \"/home/elias/PycharmProjects/aideml/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\nTypeError: Booster.predict() got an unexpected keyword argument 'ntree_limit'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000566 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3899\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3887\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000608 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3876\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000642 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3881\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 80\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3885\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13537\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4173\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 80\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3880\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3868\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3857\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3861\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3866\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13539\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000590 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4154\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 79\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000576 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3880\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.030658\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000698 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3868\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.016898\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3857\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 77\n[LightGBM] [Info] Start training from score 12.022759\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3860\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 79\n[LightGBM] [Info] Start training from score 12.027933\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3866\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 78\n[LightGBM] [Info] Start training from score 12.022040\nCV RMSE: 0.13394\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4154\n[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 79\n[LightGBM] [Info] Start training from score 12.024057\nExecution time: a moment seconds (time limit is an hour)."], "analysis": ["The training script ran successfully without any errors or bugs. The model was trained using 5-fold cross-validation on the log-transformed target, achieving a CV RMSE of approximately 0.13718, which indicates a good predictive performance on the validation folds. The final model was then trained on the full dataset and predictions were generated for the test set.", "The training script ran successfully without any errors. The model achieved a cross-validation RMSE of approximately 0.14813 on the logarithmic scale, indicating good predictive performance for the house price prediction task. The final model was trained on the full dataset and predictions were generated for the test set accordingly.", "The code execution failed due to a TypeError in the XGBRegressor's fit method. The error message indicates that the 'eval_metric' argument is not recognized, which suggests a mismatch between the code and the installed version of XGBoost. This prevented training and evaluation from completing.", "The code execution failed because the 'catboost' module is not installed in the environment, resulting in a ModuleNotFoundError. As a result, no training or prediction was performed, and no validation metric was obtained.", "The code failed to run due to a TypeError in the XGBRegressor.fit() method. The error message indicates that the 'eval_metric' argument is unexpected. This suggests that the version of XGBoost installed does not support the 'eval_metric' parameter in the fit method or there is an incompatibility. To fix this, verify the XGBoost version and update it if necessary, or remove the 'eval_metric' argument from the fit call. The rest of the code logic appears correct for training and evaluation with cross-validation and final model training.", "The code failed due to a TypeError in the XGBRegressor.fit() method. The error message indicates that the 'early_stopping_rounds' argument is unexpected. This suggests that the installed XGBoost version might not support this argument or the method signature has changed. To fix this, verify the XGBoost version and update it to a version that supports early stopping with the 'early_stopping_rounds' parameter, or remove this argument and implement early stopping differently.", "The training and evaluation script executed successfully without errors. The 5-fold cross-validation yielded a log-scale RMSE of approximately 0.13721, indicating a reasonably good model fit for the house price prediction task. The final model was trained on the full dataset and predictions for the test set were generated and saved correctly.", "The training script executed successfully without any errors. The 5-fold cross-validation using LightGBM achieved a CV RMSE of approximately 0.13505 on the log-transformed target, indicating a reasonably good model fit. The final model was trained on the full dataset and predictions were made for the test set.", "The model training and evaluation ran successfully without any errors. The 5-fold cross-validation yielded a CV RMSE of 0.13560 on the logarithm of the sale prices, indicating a good predictive performance. The full model was trained on the entire dataset and predictions were generated for the test set as expected.", "The code fails to run due to a TypeError in the xgboost model's fit method. The error indicates that the 'callbacks' argument is not recognized by XGBModel.fit(), which suggests an API mismatch or version incompatibility. To fix this, the 'callbacks' parameter should be removed or replaced with the appropriate early stopping parameter such as 'early_stopping_rounds=50'.", "The training script ran successfully without errors. The 5-fold cross-validation yielded a CV RMSE of approximately 0.13447 on the log-transformed sale price, indicating a reasonably good predictive performance. The final model was trained on the full dataset and predictions were generated for the test set for submission.", "The training script ran successfully without any errors or bugs. The model was trained using 5-fold cross-validation with LightGBM, and the out-of-fold RMSE on the log-transformed target was approximately 0.13407, indicating a good predictive performance. The final model was trained on the full dataset and predictions were generated for the test set as expected.", "The code raises a TypeError because the XGBRegressor's fit method does not accept the 'early_stopping_rounds' argument. This indicates a version mismatch or improper use of the XGBoost API. To fix this, ensure the XGBoost version supports early stopping or use the 'xgb.train' API with a DMatrix for early stopping, or update the XGBoost package. The code does not run successfully, so no validation metric is available.", "The training script executed successfully without errors. The model used LightGBM with 5-fold cross-validation and out-of-fold target encoding for the Neighborhood feature. The achieved CV RMSE on the log-transformed target was approximately 0.13564, indicating a good predictive performance. The final model was trained on the full dataset and predictions were generated for the test set.", "The training script ran successfully without any errors. The 5-fold cross-validation yielded a CV RMSE of approximately 0.13557 on the log-transformed sale prices, which is a reasonable result indicating good predictive performance. The final model was trained on the full dataset and predictions were generated for the test set for submission.", "The code raised a TypeError because the LGBMRegressor.fit() method does not accept the 'early_stopping_rounds' keyword argument. This is likely due to using a version of LightGBM where early stopping is handled differently, such as through callbacks or a different API. To fix this, the early stopping parameter should be passed via the 'callbacks' argument or by using the 'fit' method's 'eval_set' and 'eval_metric' without 'early_stopping_rounds', or by using the native LightGBM API (lgb.train) instead.", "The code raised a TypeError because the 'ntree_limit' argument is no longer supported in the 'Booster.predict()' method in the current xgboost version. The fix is to remove the 'ntree_limit' argument from the predict calls, as the model will use the best iteration internally when early stopping is enabled.", "The training script executed successfully without any errors. The model was trained using 5-fold cross-validation with LightGBM, and the out-of-fold RMSE on the log-transformed target was 0.13537, indicating good predictive performance. The final model was trained on the full dataset and predictions were generated for the test set.", "The code executed successfully without any errors. The model was trained using 5-fold cross-validation with LightGBM, and the CV RMSE on the log-transformed target was 0.13539, indicating good predictive performance. The final model was trained on the full dataset and predictions for the test set were generated and saved properly.", "The training script executed successfully without any errors. The model was trained using 5-fold cross-validation with LightGBM, achieving a CV RMSE of approximately 0.134 on the log-transformed target. This indicates a reasonably good fit for the house price prediction task. The final model was trained on the full dataset and predictions for the test set were generated and saved properly."], "exp_name": "2-translucent-spaniel-of-promotion", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
